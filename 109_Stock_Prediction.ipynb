{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Aug 30 19:31:07 2014\n",
    "@author: francesco\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "import operator\n",
    "import Quandl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas.io.data\n",
    "from sklearn.qda import QDA\n",
    "import datetime\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Methods\n",
    "def loadDatasets(path_directory): \n",
    "    \"\"\"\n",
    "    import into dataframe all datasets saved in path_directory\n",
    "    \"\"\"\n",
    "    #name = path_directory + '/procter.csv'\n",
    "    #out = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/sp.csv'\n",
    "    sp = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/nasdaq.csv'\n",
    "    nasdaq = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/djia.csv'\n",
    "    djia = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    #name = path_directory + '/treasury.csv'\n",
    "    #treasury = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/hkong.csv'\n",
    "    hkong = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/frankfurt.csv'\n",
    "    frankfurt = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/paris.csv'\n",
    "    paris = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/nikkei.csv'\n",
    "    nikkei = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/london.csv'\n",
    "    london = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/australia.csv'\n",
    "    australia = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    #return [sp, nasdaq, djia, treasury, hkong, frankfurt, paris, nikkei, london, australia]\n",
    "    #return [out, nasdaq, djia, frankfurt, hkong, nikkei, australia]\n",
    "    return [out, nasdaq, djia, frankfurt, london, paris, hkong, nikkei, australia]\n",
    "    \n",
    "\n",
    "def count_missing(dataframe):\n",
    "    \"\"\"\n",
    "    count number of NaN in dataframe\n",
    "    \"\"\"\n",
    "    return (dataframe.shape[0] * dataframe.shape[1]) - dataframe.count().sum()\n",
    "\n",
    "    \n",
    "def addFeatures(dataframe, adjclose, returns, n):\n",
    "    \"\"\"\n",
    "    operates on two columns of dataframe:\n",
    "    - n >= 2\n",
    "    - given Return_* computes the return of day i respect to day i-n. \n",
    "    - given AdjClose_* computes its moving average on n days\n",
    "    \"\"\"\n",
    "    \n",
    "    return_n = adjclose[9:] + \"Time\" + str(n)\n",
    "    dataframe[return_n] = dataframe[adjclose].pct_change(n)\n",
    "    \n",
    "    roll_n = returns[7:] + \"RolMean\" + str(n)\n",
    "    dataframe[roll_n] = pd.rolling_mean(dataframe[returns], n)\n",
    "    \n",
    "def mergeDataframes(datasets, index, target):\n",
    "    \"\"\"\n",
    "    merges datasets in the list \n",
    "    \"\"\"\n",
    "    subset = []\n",
    "    subset = [dataset.iloc[:, index:] for dataset in datasets[1:]]\n",
    "    \n",
    "    if target == 'CLASSIFICATION':    \n",
    "        return datasets[0].iloc[:, index:].join(subset, how = 'outer')\n",
    "    #elif target == 'REGRESSION':\n",
    "    #    return datasets[0].iloc[:, index:].join(subset, how = 'outer')          \n",
    "        \n",
    "# def mergeDataframes(datasets, index, cut):\n",
    "#     \"\"\"\n",
    "#     merges datasets in the list \n",
    "#     \"\"\"\n",
    "#     subset = []\n",
    "#     subset = [dataset.iloc[:, index:] for dataset in datasets[1:]]\n",
    "    \n",
    "#     first = subset[0].merge(subset[1:], how = 'outer')\n",
    "#     finance = datasets[0].iloc[:, index:].merge(first, how = 'left') \n",
    "#     finance = finance[finance.index > cut]\n",
    "#     return finance\n",
    "\n",
    "def applyTimeLag(dataset, lags, delta, back, target):\n",
    "    \"\"\"\n",
    "    apply time lag to return columns selected according  to delta.\n",
    "    Days to lag are contained in the lads list passed as argument.\n",
    "    Returns a NaN free dataset obtained cutting the lagged dataset\n",
    "    at head and tail\n",
    "    \"\"\"\n",
    "    \n",
    "    if target == 'CLASSIFICATION':\n",
    "        maxLag = max(lags)\n",
    "\n",
    "        columns = dataset.columns[::(2*max(delta)-1)]\n",
    "        for column in columns:\n",
    "            for lag in lags:\n",
    "                newcolumn = column + str(lag)\n",
    "                dataset[newcolumn] = dataset[column].shift(lag)\n",
    "\n",
    "        return dataset.iloc[maxLag:-1,:]\n",
    "#    elif target == 'REGRESSION':\n",
    "#        maxLag = max(lags)\n",
    "#        \n",
    "#        columns = dataset.columns[::(2*max(delta)-1)]\n",
    "#        for column in columns:\n",
    "#            for lag in lags:\n",
    "#                newcolumn = column + str(lag)\n",
    "#                dataset[newcolumn] = dataset[column].shift(lag)\n",
    "#\n",
    "#        return dataset.iloc[maxLag:-1,:]       \n",
    "\n",
    "\n",
    "def performCV(X_train, y_train, folds, method, parameters):\n",
    "    \"\"\"\n",
    "    given complete dataframe, number of folds, the % split to generate \n",
    "    train and test set and features to perform prediction --> splits\n",
    "    dataframein test and train set. Takes train set and splits in k folds.\n",
    "    - Train on fold 1, test on 2\n",
    "    - Train on fold 1-2, test on 3\n",
    "    - Train on fold 1-2-3, test on 4\n",
    "    ....\n",
    "    returns mean of test accuracies\n",
    "    \"\"\"\n",
    "    print ''\n",
    "    print 'Parameters --------------------------------> ', parameters\n",
    "    print 'Size train set: ', X_train.shape\n",
    "    k = int(np.floor(float(X_train.shape[0])/folds))\n",
    "    print 'Size of each fold: ', k\n",
    "    acc = np.zeros(folds-1)\n",
    "    for i in range(2, folds+1):\n",
    "        print ''\n",
    "        split = float(i-1)/i\n",
    "        print 'Splitting the first ' + str(i) + ' chuncks at ' + str(i-1) + '/' + str(i) \n",
    "        data = X_train[:(k*i)]\n",
    "        output = y_train[:(k*i)]\n",
    "        print 'Size of train+test: ', data.shape\n",
    "        index = int(np.floor(data.shape[0]*split))\n",
    "        X_tr = data[:index]        \n",
    "        y_tr = output[:index]\n",
    "        \n",
    "        X_te = data[(index+1):]\n",
    "        y_te = output[(index+1):]        \n",
    "        \n",
    "        acc[i-2] = performClassification(X_tr, y_tr, X_te, y_te, method, parameters)\n",
    "        print 'Accuracy on fold ' + str(i) + ': ', acc[i-2]\n",
    "    return acc.mean()   \n",
    "\n",
    "def performTimeSeriesSearchGrid(X_train, y_train, folds, method, grid):\n",
    "    \"\"\"\n",
    "    parameters is a dictionary with: keys --> parameter , values --> list of values of parameter\n",
    "    \"\"\"\n",
    "    print ''\n",
    "    print 'Performing Search Grid CV...'\n",
    "    print 'Algorithm: ', method\n",
    "    param = grid.keys()\n",
    "    finalGrid = {}\n",
    "    if len(param) == 1:\n",
    "        for value_0 in grid[param[0]]:\n",
    "            parameters = [value_0]\n",
    "            accuracy = performCV(dataset, folds, split, features, method, parameters)\n",
    "            finalGrid[accuracy] = parameters\n",
    "        final = sorted(finalGrid.iteritems(), key=operator.itemgetter(0), reverse=True)  \n",
    "        print ''\n",
    "        print 'Final CV Results: ', final        \n",
    "        return final[0]\n",
    "        \n",
    "    elif len(param) == 2:\n",
    "        for value_0 in grid[param[0]]:\n",
    "            for value_1 in grid[param[1]]:\n",
    "                parameters = [value_0, value_1]\n",
    "                accuracy = performCV(dataset, folds, split, features, method, parameters)\n",
    "                finalGrid[accuracy] = parameters\n",
    "        final = sorted(finalGrid.iteritems(), key=operator.itemgetter(0), reverse=True)\n",
    "        print ''\n",
    "        print 'Final CV Results: ', final\n",
    "        return final[0]\n",
    "\n",
    "\n",
    "##################\n",
    "################## MERGING SENTIMENT\n",
    "\n",
    "def mergeSentimenToStocks(stocks):\n",
    "    df = pd.read_csv('/home/francesco/BigData/Project/CSV/sentiment.csv', index_col = 'date')\n",
    "    final = stocks.join(df, how='left')\n",
    "    return final\n",
    "       \n",
    "        \n",
    "###############################################################################    \n",
    "###############################################################################    \n",
    "###############################################################################\n",
    "######## CLASSIFICATION    \n",
    "    \n",
    "#####IDEAS --> MULTIPLYEACH RETURN BY 100, QDA, AUC\n",
    "#####    \n",
    "    \n",
    "def prepareDataForClassification(dataset, start_test):\n",
    "    \"\"\"\n",
    "    generates categorical to be predicted column, attach to dataframe \n",
    "    and label the categories\n",
    "    \"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    dataset['UpDown'] = dataset['Return_Out']\n",
    "    dataset.UpDown[dataset.UpDown >= 0] = 'Up'\n",
    "    dataset.UpDown[dataset.UpDown < 0] = 'Down'\n",
    "    dataset.UpDown = le.fit(dataset.UpDown).transform(dataset.UpDown)\n",
    "    \n",
    "    features = dataset.columns[1:-1]\n",
    "    X = dataset[features]    \n",
    "    y = dataset.UpDown    \n",
    "    \n",
    "    X_train = X[X.index < start_test]\n",
    "    y_train = y[y.index < start_test]    \n",
    "    \n",
    "    X_test = X[X.index >= start_test]    \n",
    "    y_test = y[y.index >= start_test]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test    \n",
    "\n",
    "def prepareDataForModelSelection(X_train, y_train, start_validation):\n",
    "    \"\"\"\n",
    "    gets train set and generates a validation set splitting the train.\n",
    "    The validation set is mandatory for feature and model selection.\n",
    "    \"\"\"\n",
    "    X = X_train[X_train.index < start_validation]\n",
    "    y = y_train[y_train.index < start_validation]    \n",
    "    \n",
    "    X_val = X_train[X_train.index >= start_validation]    \n",
    "    y_val = y_train[y_train.index >= start_validation]   \n",
    "    \n",
    "    return X, y, X_val, y_val\n",
    "    \n",
    "  \n",
    "def performClassification(X_train, y_train, X_test, y_test, method, parameters):\n",
    "    \"\"\"\n",
    "    performs classification on returns using serveral algorithms\n",
    "    \"\"\"\n",
    "    #print ''\n",
    "    print 'Performing ' + method + ' Classification...'    \n",
    "    print 'Size of train set: ', X_train.shape\n",
    "    print 'Size of test set: ', X_test.shape\n",
    "   \n",
    "    if method == 'RF':   \n",
    "        return performRFClass(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "    elif method == 'KNN':\n",
    "        return performKNNClass(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif method == 'SVM':   \n",
    "        return performSVMClass(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif method == 'ADA':\n",
    "        return performAdaBoostClass(X_train, y_train, X_test, y_test, parameters)\n",
    "    \n",
    "    elif method == 'GTB': \n",
    "        return performGTBClass(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    elif method == 'QDA': \n",
    "        return performQDAClass(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "def performRFClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Binary Classification\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    #auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "    return accuracy\n",
    "        \n",
    "def performKNNClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    KNN binary Classification\n",
    "    \"\"\"\n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    #auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "    return accuracy\n",
    "\n",
    "def performSVMClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    SVM binary Classification\n",
    "    \"\"\"\n",
    "    clf = SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    #auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "    return accuracy\n",
    "    \n",
    "def performAdaBoostClass(X_train, y_train, X_test, y_test, parameters):\n",
    "    \"\"\"\n",
    "    Ada Boosting binary Classification\n",
    "    \"\"\"\n",
    "    n = parameters[0]\n",
    "    l =  parameters[1]\n",
    "    clf = AdaBoostClassifier(n_estimators = n, learning_rate = l)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    #auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "    return accuracy\n",
    "    \n",
    "def performGTBClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Tree Boosting binary Classification\n",
    "    \"\"\"\n",
    "    clf = GradientBoostingClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    #auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "    return accuracy\n",
    "\n",
    "def performQDAClass(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Tree Boosting binary Classification\n",
    "    \"\"\"\n",
    "    clf = QDA()\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    #auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################   \n",
    "##############################################################################\n",
    "####### REGRESSION\n",
    "    \n",
    "def performRegression(dataset, split):\n",
    "    \"\"\"\n",
    "    performs regression on returns using serveral algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    features = dataset.columns[1:]\n",
    "    index = int(np.floor(dataset.shape[0]*split))\n",
    "    train, test = dataset[:index], dataset[index:]\n",
    "    print 'Size of train set: ', train.shape\n",
    "    print 'Size of test set: ', test.shape\n",
    "    \n",
    "    output = 'Return_SP500'\n",
    "\n",
    "    #print 'Accuracy RFC: ', performRFReg(train, test, features, output)\n",
    "   \n",
    "    #print 'Accuracy SVM: ', performSVMReg(train, test, features, output)\n",
    "   \n",
    "    #print 'Accuracy BAG: ', performBaggingReg(train, test, features, output)\n",
    "   \n",
    "    #print 'Accuracy ADA: ', performAdaBoostReg(train, test, features, output)\n",
    "   \n",
    "    #print 'Accuracy BOO: ', performGradBoostReg(train, test, features, output)\n",
    "\n",
    "    print 'Accuracy KNN: ', performKNNReg(train, test, features, output)\n",
    "\n",
    "\n",
    "def performRFReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Random Forest Regression\n",
    "    \"\"\"\n",
    "\n",
    "    forest = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    forest = forest.fit(train[features], train[output])\n",
    "    Predicted = forest.predict(test[features])\n",
    "    \n",
    "\n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output], Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def performSVMReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    SVM Regression\n",
    "    \"\"\"\n",
    "\n",
    "    clf = SVR()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "    \n",
    "def performBaggingReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Bagging Regression\n",
    "    \"\"\"\n",
    "  \n",
    "    clf = BaggingRegressor()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)  \n",
    "\n",
    "def performAdaBoostReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Ada Boost Regression\n",
    "    \"\"\"\n",
    "\n",
    "    clf = AdaBoostRegressor()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def performGradBoostReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regression\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = GradientBoostingRegressor()\n",
    "    clf.fit(test[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()    \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "\n",
    "def performKNNReg(train, test, features, output):\n",
    "    \"\"\"\n",
    "    KNN Regression\n",
    "    \"\"\"\n",
    "\n",
    "    clf = KNeighborsRegressor()\n",
    "    clf.fit(train[features], train[output])\n",
    "    Predicted = clf.predict(test[features])\n",
    "    \n",
    "    plt.plot(test[output])\n",
    "    plt.plot(Predicted, color='red')\n",
    "    plt.show()        \n",
    "    \n",
    "    return mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def getStock(symbol, start, end):\n",
    "#     \"\"\"\n",
    "#     downloads stock which is gonna be the output of prediciton\n",
    "#     \"\"\"\n",
    "#     out =  pd.io.data.DataReader(symbol, start, end)\n",
    "#     out = out.to_frame()\n",
    "#     out.columns.values[-1] = 'AdjClose'\n",
    "#     out.columns = out.columns + '_Out'\n",
    "#     out['Return_Out'] = out['AdjClose_Out'].pct_change()\n",
    "#     return out\n",
    "\n",
    "def getStock(symbol, start, end):\n",
    "    \"\"\"\n",
    "    downloads stock which is gonna be the output of prediciton\n",
    "    \"\"\"\n",
    "    out =  pd.io.data.get_data_yahoo(symbol, start, end)\n",
    "\n",
    "    out.columns.values[-1] = 'AdjClose'\n",
    "    out.columns = out.columns + '_Out'\n",
    "    out['Return_Out'] = out['AdjClose_Out'].pct_change()\n",
    "    return out\n",
    "\n",
    "def loadDatasets(path_directory): \n",
    "    \"\"\"\n",
    "    import into dataframe all datasets saved in path_directory\n",
    "    \"\"\"\n",
    "    #name = path_directory + '/procter.csv'\n",
    "    #out = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/sp.csv'\n",
    "    sp = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/nasdaq.csv'\n",
    "    nasdaq = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/djia.csv'\n",
    "    djia = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/treasury.csv'\n",
    "    treasury = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/hkong.csv'\n",
    "    hkong = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/frankfurt.csv'\n",
    "    frankfurt = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/paris.csv'\n",
    "    paris = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/nikkei.csv'\n",
    "    nikkei = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/london.csv'\n",
    "    london = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    name = path_directory + '/australia.csv'\n",
    "    australia = pd.read_csv(name, index_col=0, parse_dates=True)\n",
    "    \n",
    "    return [sp, nasdaq, djia, treasury, hkong, frankfurt, paris, nikkei, london, australia]\n",
    "\n",
    "def applyRollMeanDelayedReturns(datasets, delta):\n",
    "    for dataset in datasets:\n",
    "        columns = dataset.columns    \n",
    "        adjclose = columns[-2]\n",
    "        returns = columns[-1]\n",
    "        for n in delta:\n",
    "            addFeatures(dataset, adjclose, returns, n)\n",
    "        \n",
    "\n",
    "def performFeatureSelection(maxdeltas, maxlags, cut, start_test, path_datasets, method, folds, parameters):\n",
    "    \"\"\"\n",
    "    Performs Feature selection for a specific algorithm\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for maxlag in range(3, maxlags + 2):\n",
    "        lags = range(2, maxlag) \n",
    "        print ''\n",
    "        print '============================================================='\n",
    "        print 'Maximum time lag applied', max(lags)\n",
    "        print ''\n",
    "        for maxdelta in range(3, maxdeltas + 2):\n",
    "            datasets = loadDatasets(path_datasets)\n",
    "            delta = range(2, maxdelta) \n",
    "            print 'Delta days accounted: ', max(delta)\n",
    "            \n",
    "            start = datetime.datetime(1993, 1, 1)\n",
    "            end = datetime.datetime(2015, 8, 31)\n",
    "            out = getStock('GE', start, end)\n",
    "            datasets.insert(0, out)  \n",
    "            applyRollMeanDelayedReturns(datasets, delta)\n",
    "            \n",
    "            #finance = mergeDataframes(datasets, 6, cut)\n",
    "            finance = mergeDataframes(datasets, 6, \"CLASSIFICATION\")\n",
    "            print 'Size of data frame: ', finance.shape\n",
    "            print 'Number of NaN after merging: ', count_missing(finance)\n",
    "            finance = finance.interpolate(method='linear')\n",
    "            print 'Number of NaN after time interpolation: ', count_missing(finance)\n",
    "            finance = finance.fillna(finance.mean())\n",
    "            print 'Number of NaN after mean interpolation: ', count_missing(finance)    \n",
    "            \n",
    "            back = -1\n",
    "            finance.Return_Out = finance.Return_Out.shift(back)\n",
    "#TIME LAG???  \n",
    "            finance = applyTimeLag(finance, lags, delta, back, target)\n",
    "\n",
    "            #finance = applyTimeLag(finance, lags, delta)\n",
    "            print 'Number of NaN after temporal shifting: ', count_missing(finance)\n",
    "            print 'Size of data frame after feature creation: ', finance.shape\n",
    "            X_train, y_train, X_test, y_test  = prepareDataForClassification(finance, start_test)\n",
    "            accuracy = performCV(X_train, y_train, folds, method, parameters)\n",
    "            print accuracy\n",
    "            accuracies.append((lags,delta,accuracy))\n",
    "            print ''\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'CLASSIFICATION'\n",
    "#We take from 1993-2015 to get better results from interpolate and fillna\n",
    "#After replacing nan, we take 2013-2015\n",
    "start = datetime.datetime(1993, 1, 1)\n",
    "end = datetime.datetime(2015, 11, 30)\n",
    "#Get stock for S&P500\n",
    "a=getStock('^GSPC', start, end)\n",
    "out = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nan: 441\n",
      "number of nan after interpolate: 0\n",
      "number of nan after fillna: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_price</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>volume</th>\n",
       "      <th>last_price</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>volume</th>\n",
       "      <th>last_price</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>volume</th>\n",
       "      <th>last_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>112.5213</td>\n",
       "      <td>113.68</td>\n",
       "      <td>113.48</td>\n",
       "      <td>5986856</td>\n",
       "      <td>1453.6</td>\n",
       "      <td>1454.000</td>\n",
       "      <td>1454.000</td>\n",
       "      <td>16499</td>\n",
       "      <td>1459.37</td>\n",
       "      <td>1460.48</td>\n",
       "      <td>1457.68</td>\n",
       "      <td>561300032</td>\n",
       "      <td>14.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>110.2000</td>\n",
       "      <td>111.04</td>\n",
       "      <td>110.84</td>\n",
       "      <td>6891865</td>\n",
       "      <td>1457.7</td>\n",
       "      <td>1454.675</td>\n",
       "      <td>1454.675</td>\n",
       "      <td>9099</td>\n",
       "      <td>1466.47</td>\n",
       "      <td>1472.26</td>\n",
       "      <td>1463.79</td>\n",
       "      <td>489478816</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>110.0800</td>\n",
       "      <td>108.84</td>\n",
       "      <td>108.72</td>\n",
       "      <td>5772071</td>\n",
       "      <td>1455.8</td>\n",
       "      <td>1456.700</td>\n",
       "      <td>1456.700</td>\n",
       "      <td>13730</td>\n",
       "      <td>1461.89</td>\n",
       "      <td>1462.21</td>\n",
       "      <td>1459.36</td>\n",
       "      <td>471509120</td>\n",
       "      <td>13.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>108.7200</td>\n",
       "      <td>108.28</td>\n",
       "      <td>108.20</td>\n",
       "      <td>6761085</td>\n",
       "      <td>1452.3</td>\n",
       "      <td>1451.600</td>\n",
       "      <td>1449.700</td>\n",
       "      <td>19369</td>\n",
       "      <td>1457.15</td>\n",
       "      <td>1460.41</td>\n",
       "      <td>1455.11</td>\n",
       "      <td>533650944</td>\n",
       "      <td>13.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-09</th>\n",
       "      <td>108.9600</td>\n",
       "      <td>108.36</td>\n",
       "      <td>108.12</td>\n",
       "      <td>5460468</td>\n",
       "      <td>1455.8</td>\n",
       "      <td>1456.500</td>\n",
       "      <td>1456.500</td>\n",
       "      <td>9210</td>\n",
       "      <td>1461.02</td>\n",
       "      <td>1464.34</td>\n",
       "      <td>1459.44</td>\n",
       "      <td>489918240</td>\n",
       "      <td>13.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            last_price     ask     bid   volume  last_price       ask  \\\n",
       "2013-01-03    112.5213  113.68  113.48  5986856      1453.6  1454.000   \n",
       "2013-01-04    110.2000  111.04  110.84  6891865      1457.7  1454.675   \n",
       "2013-01-07    110.0800  108.84  108.72  5772071      1455.8  1456.700   \n",
       "2013-01-08    108.7200  108.28  108.20  6761085      1452.3  1451.600   \n",
       "2013-01-09    108.9600  108.36  108.12  5460468      1455.8  1456.500   \n",
       "\n",
       "                 bid  volume  last_price      ask      bid     volume  \\\n",
       "2013-01-03  1454.000   16499     1459.37  1460.48  1457.68  561300032   \n",
       "2013-01-04  1454.675    9099     1466.47  1472.26  1463.79  489478816   \n",
       "2013-01-07  1456.700   13730     1461.89  1462.21  1459.36  471509120   \n",
       "2013-01-08  1449.700   19369     1457.15  1460.41  1455.11  533650944   \n",
       "2013-01-09  1456.500    9210     1461.02  1464.34  1459.44  489918240   \n",
       "\n",
       "            last_price  \n",
       "2013-01-03       14.56  \n",
       "2013-01-04       13.83  \n",
       "2013-01-07       13.79  \n",
       "2013-01-08       13.62  \n",
       "2013-01-09       13.81  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data from bloomberg\n",
    "path = \"/Users/Kevin/Desktop/project/\"\n",
    "sp1 = pd.read_excel(path+'SP1_index.xlsx', index_col=0, parse_dates=True)\n",
    "spx = pd.read_excel(path+'SPX_index.xlsx', index_col=0, parse_dates=True)\n",
    "vix = pd.read_excel(path+'VIX_index.xlsx', index_col=0, parse_dates=True)\n",
    "vxx = pd.read_excel(path+'VXX_equity.xlsx', index_col=0, parse_dates=True)\n",
    "\n",
    "#Merge data from bloomberg\n",
    "test = pd.concat([vxx,sp1,spx,vix],axis=1)\n",
    "print 'number of nan:',count_missing(test)\n",
    "test = test.interpolate(method='time')\n",
    "print 'number of nan after interpolate:',count_missing(test)\n",
    "test = test.fillna(test.mean())\n",
    "print 'number of nan after fillna:',count_missing(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters -------------------------------->  []\n",
      "Size train set:  (311, 45)\n",
      "Size of each fold:  31\n",
      "\n",
      "Splitting the first 2 chuncks at 1/2\n",
      "Size of train+test:  (62, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (31, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 2:  0.366666666667\n",
      "\n",
      "Splitting the first 3 chuncks at 2/3\n",
      "Size of train+test:  (93, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (62, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 3:  0.766666666667\n",
      "\n",
      "Splitting the first 4 chuncks at 3/4\n",
      "Size of train+test:  (124, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (93, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 4:  0.633333333333\n",
      "\n",
      "Splitting the first 5 chuncks at 4/5\n",
      "Size of train+test:  (155, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (124, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 5:  0.566666666667\n",
      "\n",
      "Splitting the first 6 chuncks at 5/6\n",
      "Size of train+test:  (186, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (155, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 6:  0.566666666667\n",
      "\n",
      "Splitting the first 7 chuncks at 6/7\n",
      "Size of train+test:  (217, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (186, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 7:  0.4\n",
      "\n",
      "Splitting the first 8 chuncks at 7/8\n",
      "Size of train+test:  (248, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (217, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 8:  0.5\n",
      "\n",
      "Splitting the first 9 chuncks at 8/9\n",
      "Size of train+test:  (279, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (248, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 9:  0.533333333333\n",
      "\n",
      "Splitting the first 10 chuncks at 9/10\n",
      "Size of train+test:  (310, 45)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (279, 45)\n",
      "Size of test set:  (30, 45)\n",
      "Accuracy on fold 10:  0.433333333333\n",
      "Average accuracy after cross validation using Bloomberg data: 0.52962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Kevin/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Fill in nan in dataframe\n",
    "outa = out.interpolate(method='time')\n",
    "#print 'Number of NaN after time interpolation: ', count_missing(out)\n",
    "outa = outa.fillna(out.mean())\n",
    "#print 'Number of NaN after mean interpolation: ', count_missing(out)    \n",
    "\n",
    "#Our bloomberg data starts at 2013, but data from yahoo starts at 1993. So we start at 2013\n",
    "start = datetime.datetime(2013,1,3)\n",
    "end = datetime.datetime(2015,11,27)\n",
    "outa = outa[outa.index > start]\n",
    "outa = outa[outa.index < end]\n",
    "\n",
    "#Add column UpDown. Convert returns_out to 1 if positive else 0\n",
    "le = preprocessing.LabelEncoder()    \n",
    "outa['UpDown'] = outa['Return_Out']\n",
    "outa.UpDown[outa.Return_Out >= 0] = 'Up'\n",
    "outa.UpDown[outa.Return_Out < 0] = 'Down'\n",
    "outa.UpDown = le.fit(outa.UpDown).transform(outa.UpDown)\n",
    "outa = outa.join(test)\n",
    "del outa['Return_Out']\n",
    "#Where to split train, test data\n",
    "start_test = datetime.datetime(2014,4,1)\n",
    "#All columns are features except UpDown, this is a bad way to do it... UpDown is the 7th column\n",
    "features = outa.columns[[True]*6+[False]+[True]*12]\n",
    "\n",
    "X = outa[features]    \n",
    "y = outa.UpDown    \n",
    "X_train = X[X.index < start_test]\n",
    "y_train = y[y.index < start_test]    \n",
    "X_test = X[X.index >= start_test]    \n",
    "y_test = y[y.index >= start_test]\n",
    "avg_accuracy = performCV(X_train, y_train, 10, \"RF\", [])\n",
    "print 'Average accuracy after cross validation using Bloomberg data:',avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN after time interpolation:  1\n",
      "Number of NaN after mean interpolation:  0\n",
      "\n",
      "Parameters -------------------------------->  []\n",
      "Size train set:  (5350, 6)\n",
      "Size of each fold:  535\n",
      "\n",
      "Splitting the first 2 chuncks at 1/2\n",
      "Size of train+test:  (1070, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (535, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 2:  0.576779026217\n",
      "\n",
      "Splitting the first 3 chuncks at 2/3\n",
      "Size of train+test:  (1605, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (1070, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 3:  0.468164794007\n",
      "\n",
      "Splitting the first 4 chuncks at 3/4\n",
      "Size of train+test:  (2140, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (1605, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 4:  0.644194756554\n",
      "\n",
      "Splitting the first 5 chuncks at 4/5\n",
      "Size of train+test:  (2675, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (2140, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 5:  0.702247191011\n",
      "\n",
      "Splitting the first 6 chuncks at 5/6\n",
      "Size of train+test:  (3210, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (2675, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 6:  0.797752808989\n",
      "\n",
      "Splitting the first 7 chuncks at 6/7\n",
      "Size of train+test:  (3745, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (3210, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 7:  0.760299625468\n",
      "\n",
      "Splitting the first 8 chuncks at 7/8\n",
      "Size of train+test:  (4280, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (3745, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 8:  0.846441947566\n",
      "\n",
      "Splitting the first 9 chuncks at 8/9\n",
      "Size of train+test:  (4815, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (4280, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 9:  0.89138576779\n",
      "\n",
      "Splitting the first 10 chuncks at 9/10\n",
      "Size of train+test:  (5350, 6)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (4815, 6)\n",
      "Size of test set:  (534, 6)\n",
      "Accuracy on fold 10:  0.692883895131\n",
      "Average accuracy after CV using only attributes of s&p500 from yahoo: 0.708905534748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Kevin/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Using only attributes of S&P500 from yahoo\n",
    "\n",
    "outb = out.interpolate(method='time')\n",
    "print 'Number of NaN after time interpolation: ', count_missing(outb)\n",
    "outb = outb.fillna(outb.mean())\n",
    "print 'Number of NaN after mean interpolation: ', count_missing(outb)    \n",
    "start_test = datetime.datetime(2014,4,1)\n",
    "le = preprocessing.LabelEncoder()    \n",
    "outb['UpDown'] = outb['Return_Out']\n",
    "outb.UpDown[outb.Return_Out >= 0] = 'Up'\n",
    "outb.UpDown[outb.Return_Out < 0] = 'Down'\n",
    "outb.UpDown = le.fit(outb.UpDown).transform(outb.UpDown)\n",
    "\n",
    "features = outb.columns[0:-2]\n",
    "X = outb[features]    \n",
    "y = outb.UpDown    \n",
    "    \n",
    "X_train = X[X.index < start_test]\n",
    "y_train = y[y.index < start_test]    \n",
    "    \n",
    "X_test = X[X.index >= start_test]    \n",
    "y_test = y[y.index >= start_test]\n",
    "#print performClassification(X_train, y_train, X_test, y_test, \"RF\", [])\n",
    "#Methods = QDA,GTB,SVM,KNN\n",
    "avg_accuracy = performCV(X_train, y_train, 10, \"RF\", [])\n",
    "print 'Average accuracy after CV using only attributes of s&p500 from yahoo:',avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum time lag applied 10\n",
      "Max Delta days accounted:  10\n",
      "Size of data frame:  (6209, 209)\n",
      "Number of NaN after merging:  96446\n",
      "% of NaN after merging:  7.43218094432 %\n",
      "Number of NaN after time interpolation:  49636\n",
      "Number of NaN after mean interpolation:  0\n",
      "Number of NaN after temporal shifting:  0\n",
      "Size of data frame after feature creation:  (6209, 209)\n",
      "Performing RF Classification...\n",
      "Size of train set:  (5796, 208)\n",
      "Size of test set:  (413, 208)\n",
      "0.530266343826\n",
      "Performing RF Classification...\n",
      "Size of train set:  (5796, 208)\n",
      "Size of test set:  (413, 208)\n",
      "0.755447941889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:241: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Setup and load datasets\n",
    "target = 'CLASSIFICATION'\n",
    "lags = range(2, 11)\n",
    "print 'Maximum time lag applied', max(lags)\n",
    "start = datetime.datetime(1993, 1, 1)\n",
    "end = datetime.datetime(2015, 8, 31)\n",
    "out = getStock('GE', start, end)\n",
    "datasets = loadDatasets('/Users/Kevin/Desktop/StocksProject/datasets')\n",
    "\n",
    "#Insert GE stock as first dataset\n",
    "datasets.insert(0, out)    \n",
    "delta = range(2, 11)\n",
    "print 'Max Delta days accounted: ', max(delta)\n",
    "    \n",
    "#Add features to each stock, number of features added depends on deltas\n",
    "for dataset in datasets:\n",
    "    columns = dataset.columns    \n",
    "    adjclose = columns[-2]\n",
    "    returns = columns[-1]\n",
    "    for n in delta:\n",
    "        addFeatures(dataset, adjclose, returns, n)\n",
    "\n",
    "#Merge our augmented dataframes\n",
    "finance = mergeDataframes(datasets, 6, target)\n",
    "\n",
    "#Sanity check\n",
    "print 'Size of data frame: ', finance.shape\n",
    "print 'Number of NaN after merging: ', count_missing(finance)\n",
    "print '% of NaN after merging: ', (count_missing(finance)/float(finance.shape[0]*finance.shape[1]))*100, '%'\n",
    "\n",
    "#Interpolate is built-in function that replaces missing values(i.e. nan) with estimates. It is \"time aware\", so it\n",
    "#looks at the dates(i.e. index) to determine what value to fill in\n",
    "finance = finance.interpolate(method = 'time')\n",
    "print 'Number of NaN after time interpolation: ', count_missing(finance)\n",
    "\n",
    "#Still some nan left, fill with mean\n",
    "finance = finance.fillna(finance.mean())\n",
    "print 'Number of NaN after mean interpolation: ', count_missing(finance)    \n",
    "\n",
    "#Don't understand why SHIFT????\n",
    "back = -1\n",
    "#finance.Return_Out = finance.Return_Out.shift(back)\n",
    "#TIME LAG???  \n",
    "#finance = applyTimeLag(finance, lags, delta, back, target)\n",
    "#finance = functions.mergeSentimenToStocks(finance)\n",
    "\n",
    "print 'Number of NaN after temporal shifting: ', count_missing(finance)\n",
    "print 'Size of data frame after feature creation: ', finance.shape\n",
    "\n",
    "#Create train, test set. Split at start_test\n",
    "if target == 'CLASSIFICATION':\n",
    "    start_test = datetime.datetime(2014,4,1)\n",
    "    X_train, y_train, X_test, y_test  = prepareDataForClassification(finance, start_test)\n",
    "    \n",
    "#Original classifier has 40% accuracy, take the opposite of every guess to get 60% accuracy\n",
    "print performClassification(X_train, np.random.choice(2,len(y_train),p=[.65,.35]), X_test, y_test, \"RF\", [])\n",
    "print performClassification(X_train, y_train, X_test, y_test, \"RF\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/Kevin/Desktop/project/fundamendal_analysis_data/'\n",
    "#Read in CPI\n",
    "#Units = Index 1982-1984=100???\n",
    "cpi = pd.read_excel(path+'CPI.xls',index_col=0, parse_dates=True).reset_index()\n",
    "#Read in GDP\n",
    "#Units = billions\n",
    "#year|gdp|gdb of chained 2009 dollars | quarterly year| gdp | gdb of chained 2009 dollars \n",
    "gdp_yearly = pd.read_excel(path+'gdp_yearly.xls',index_col=0, parse_dates=True).reset_index()\n",
    "gdp_quaterly = pd.read_excel(path+'gdp_quarterly.xlsx',index_col=0, parse_dates=True).reset_index()\n",
    "#Read in M1\n",
    "m1 = pd.read_excel(path+'M1.xls',index_col=0, parse_dates=True).reset_index()\n",
    "#Read M2\n",
    "m2 = pd.read_excel(path+'M2.xls',index_col=0, parse_dates=True).reset_index()\n",
    "m2.observation_date = pd.Series([x.to_datetime().date() for x in m2.observation_date])\n",
    "#Read in Treasury 10/2\n",
    "#Unit percent, daily\n",
    "treasury_10_2 = pd.read_excel(path+'10_minus_2_treasury.xls',index_col=0, parse_dates=True).reset_index()\n",
    "#Read in Treasury 10\n",
    "#Unit percent\n",
    "treasury_10 = pd.read_excel(path+'10_year_treasury.xls',index_col=0, parse_dates=True).reset_index()\n",
    "treasury_10.DATE = pd.Series([x.to_datetime().date() for x in treasury_10.DATE])\n",
    "treasury_2 = pd.read_excel(path+'2_year_treasury.xls',index_col=0, parse_dates=True).reset_index()\n",
    "treasury_2.observation_date = pd.Series([x.to_datetime().date() for x in treasury_2.observation_date])\n",
    "path = '/Users/Kevin/Desktop/project/StocksProject/datasets/'\n",
    "international_indices_names = ['australia.csv','djia.csv','frankfurt.csv','hkong.csv',\\\n",
    "                              'london.csv','nasdaq.csv','nikkei.csv','paris.csv','treasury.csv']\n",
    "international_indices = []\n",
    "for i in international_indices_names:\n",
    "    tmp = pd.read_csv(path+i,parse_dates=True).reset_index()\n",
    "    tmp.Date = pd.Series([datetime.datetime.strptime(x,'%Y-%m-%d').date() for x in tmp.Date])\n",
    "    international_indices.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_date</th>\n",
       "      <th>T10Y2Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-12-06</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-12-07</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-12-08</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-12-09</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-12-10</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  observation_date  T10Y2Y\n",
       "0       2010-12-06    2.53\n",
       "1       2010-12-07    2.61\n",
       "2       2010-12-08    2.63\n",
       "3       2010-12-09    2.59\n",
       "4       2010-12-10    2.68"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treasury_10_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993-01-04\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP_Open</th>\n",
       "      <th>SP_High</th>\n",
       "      <th>SP_Low</th>\n",
       "      <th>SP_Close</th>\n",
       "      <th>SP_Volume</th>\n",
       "      <th>SP_Adj Close</th>\n",
       "      <th>YAHOO/SSRI</th>\n",
       "      <th>YAHOO/GOLD</th>\n",
       "      <th>GOOG/NYSEARCA_CORN</th>\n",
       "      <th>...</th>\n",
       "      <th>T2</th>\n",
       "      <th>AdjClose_Australia</th>\n",
       "      <th>AdjClose_Djia</th>\n",
       "      <th>AdjClose_Frankfurt</th>\n",
       "      <th>AdjClose_HKong</th>\n",
       "      <th>AdjClose_London</th>\n",
       "      <th>AdjClose_Nasdaq</th>\n",
       "      <th>AdjClose_Nikkei</th>\n",
       "      <th>AdjClose_Paris</th>\n",
       "      <th>AdjClose_Treasury</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-01-04</td>\n",
       "      <td>435.700012</td>\n",
       "      <td>437.320007</td>\n",
       "      <td>434.480011</td>\n",
       "      <td>435.380005</td>\n",
       "      <td>201210000</td>\n",
       "      <td>435.380005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.48</td>\n",
       "      <td>1576.099976</td>\n",
       "      <td>3309.2</td>\n",
       "      <td>1531.300049</td>\n",
       "      <td>5437.799805</td>\n",
       "      <td>2861.500000</td>\n",
       "      <td>671.799988</td>\n",
       "      <td>16994</td>\n",
       "      <td>1843.099976</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-01-05</td>\n",
       "      <td>435.380005</td>\n",
       "      <td>435.399994</td>\n",
       "      <td>433.549988</td>\n",
       "      <td>434.339996</td>\n",
       "      <td>240350000</td>\n",
       "      <td>434.339996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.48</td>\n",
       "      <td>1579.900024</td>\n",
       "      <td>3307.9</td>\n",
       "      <td>1556.400024</td>\n",
       "      <td>5548.000000</td>\n",
       "      <td>2833.600098</td>\n",
       "      <td>674.340027</td>\n",
       "      <td>16843</td>\n",
       "      <td>1850.800049</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-01-06</td>\n",
       "      <td>434.339996</td>\n",
       "      <td>435.170013</td>\n",
       "      <td>432.519989</td>\n",
       "      <td>434.519989</td>\n",
       "      <td>295240000</td>\n",
       "      <td>434.519989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.52</td>\n",
       "      <td>1554.800049</td>\n",
       "      <td>3305.2</td>\n",
       "      <td>1556.400024</td>\n",
       "      <td>5586.700195</td>\n",
       "      <td>2826.000000</td>\n",
       "      <td>681.849976</td>\n",
       "      <td>16783</td>\n",
       "      <td>1859.599976</td>\n",
       "      <td>5.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993-01-07</td>\n",
       "      <td>434.519989</td>\n",
       "      <td>435.459991</td>\n",
       "      <td>429.760010</td>\n",
       "      <td>430.730011</td>\n",
       "      <td>304850000</td>\n",
       "      <td>430.730011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1542.099976</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>1542.500000</td>\n",
       "      <td>5547.299805</td>\n",
       "      <td>2816.500000</td>\n",
       "      <td>678.210022</td>\n",
       "      <td>16781</td>\n",
       "      <td>1844.500000</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-01-08</td>\n",
       "      <td>430.730011</td>\n",
       "      <td>430.730011</td>\n",
       "      <td>426.880005</td>\n",
       "      <td>429.049988</td>\n",
       "      <td>263470000</td>\n",
       "      <td>429.049988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1510.400024</td>\n",
       "      <td>3251.7</td>\n",
       "      <td>1531.500000</td>\n",
       "      <td>5529.600098</td>\n",
       "      <td>2799.199951</td>\n",
       "      <td>677.210022</td>\n",
       "      <td>16635</td>\n",
       "      <td>1852.599976</td>\n",
       "      <td>5.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     SP_Open     SP_High      SP_Low    SP_Close  SP_Volume  \\\n",
       "0  1993-01-04  435.700012  437.320007  434.480011  435.380005  201210000   \n",
       "1  1993-01-05  435.380005  435.399994  433.549988  434.339996  240350000   \n",
       "2  1993-01-06  434.339996  435.170013  432.519989  434.519989  295240000   \n",
       "3  1993-01-07  434.519989  435.459991  429.760010  430.730011  304850000   \n",
       "4  1993-01-08  430.730011  430.730011  426.880005  429.049988  263470000   \n",
       "\n",
       "   SP_Adj Close  YAHOO/SSRI  YAHOO/GOLD  GOOG/NYSEARCA_CORN  \\\n",
       "0    435.380005         NaN         NaN                 NaN   \n",
       "1    434.339996         NaN         NaN                 NaN   \n",
       "2    434.519989         NaN         NaN                 NaN   \n",
       "3    430.730011         NaN         NaN                 NaN   \n",
       "4    429.049988         NaN         NaN                 NaN   \n",
       "\n",
       "         ...            T2  AdjClose_Australia  AdjClose_Djia  \\\n",
       "0        ...          4.48         1576.099976         3309.2   \n",
       "1        ...          4.48         1579.900024         3307.9   \n",
       "2        ...          4.52         1554.800049         3305.2   \n",
       "3        ...          4.64         1542.099976         3269.0   \n",
       "4        ...          4.50         1510.400024         3251.7   \n",
       "\n",
       "   AdjClose_Frankfurt  AdjClose_HKong  AdjClose_London  AdjClose_Nasdaq  \\\n",
       "0         1531.300049     5437.799805      2861.500000       671.799988   \n",
       "1         1556.400024     5548.000000      2833.600098       674.340027   \n",
       "2         1556.400024     5586.700195      2826.000000       681.849976   \n",
       "3         1542.500000     5547.299805      2816.500000       678.210022   \n",
       "4         1531.500000     5529.600098      2799.199951       677.210022   \n",
       "\n",
       "   AdjClose_Nikkei  AdjClose_Paris  AdjClose_Treasury  \n",
       "0            16994     1843.099976               5.90  \n",
       "1            16843     1850.800049               5.90  \n",
       "2            16783     1859.599976               5.94  \n",
       "3            16781     1844.500000               6.05  \n",
       "4            16635     1852.599976               5.97  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge my dataframes, then merge with Tang's\n",
    "#stocks_tang.Date = pd.Series([x.date() for x in stocks_tang.Date])\n",
    "big_df = stocks_tang.copy()\n",
    "#Rename column to Data so we can merge onto s&p500\n",
    "a=m2.rename(columns = {'observation_date':'Date'})\n",
    "big_df = pd.merge(big_df, a, on='Date', how='left')\n",
    "print big_df.Date[0]\n",
    "a=treasury_10.rename(columns = {'DATE':'Date'})\n",
    "big_df = pd.merge(big_df, a, on='Date', how='left')\n",
    "a=treasury_2.rename(columns = {'observation_date':'Date'})\n",
    "big_df = pd.merge(big_df, a, on='Date', how='left')\n",
    "#Now merge international\n",
    "international_names = ['Australia','Djia','Frankfurt','HKong','London','Nasdaq','Nikkei','Paris','Treasury']\n",
    "for idx,i in enumerate(international_indices):\n",
    "    big_df = big_df.merge(i[['Date','AdjClose_'+international_names[idx]]],on='Date',how='left')\n",
    "\n",
    "#Rename columns of big dataframe\n",
    "big_df = big_df.rename(columns={'VALUE':'T10','DGS2':'T2'}) \n",
    "big_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "big_df.to_csv('/Users/Kevin/Desktop/project/big_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime(2012,1,1)==datetime.datetime(2012,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock(symbol, trim_start=\"1993-01-01\", trim_end=\"2015-10-31\"):\n",
    "    return Quandl.get(symbol,authtoken='c2365v55yoZrWKxbVxwK', trim_start = trim_start, trim_end = trim_end)\n",
    "\n",
    "\n",
    "def get_features_close_price(feature_labels, p_sp=None):\n",
    "    \"\"\"\n",
    "    Returns a data table concatenating all close data for the features\n",
    "    \"\"\"\n",
    "    \n",
    "    if(p_sp is None):\n",
    "        # First get SP500 data\n",
    "        p_sp = get_stock(\"YAHOO/INDEX_GSPC\")\n",
    "        print 'SP index acquired'\n",
    "        p_sp.columns = ['SP_'+i for i in p_sp.columns]\n",
    "        p_sp = p_sp.reset_index()\n",
    "    \n",
    "    for feature in feature_labels:\n",
    "        print 'READING ', feature\n",
    "        try:\n",
    "            p = get_stock(feature)\n",
    "\n",
    "            # just get the close\n",
    "            p = p.reset_index()\n",
    "\n",
    "            adj_index = p.columns[['Adj' in i for i in p.columns]]\n",
    "            close_index = p.columns[['Close' in i for i in p.columns]]\n",
    "            price_index = p.columns[['Price' in i for i in p.columns]]\n",
    "            \n",
    "            # Use adjusted close if there is \n",
    "            if len(adj_index)>0:\n",
    "                print adj_index[0]\n",
    "                temp = p[['Date', adj_index[0]]]\n",
    "                temp.columns = ['Date', feature]\n",
    "                p_sp = pd.merge(p_sp, temp, on='Date', how='left')\n",
    "                \n",
    "            # otherwise use simple close\n",
    "            elif len(close_index)>0:\n",
    "                print close_index[0]\n",
    "                temp = p[['Date', close_index[0]]]\n",
    "                temp.columns = ['Date', feature]\n",
    "                p_sp = pd.merge(p_sp, temp, on='Date', how='left')\n",
    "                \n",
    "            # or just price\n",
    "            elif len(price_index)>0:\n",
    "                print price_index[0]\n",
    "                temp = p[['Date', price_index[0]]]\n",
    "                temp.columns = ['Date', feature]\n",
    "                p_sp = pd.merge(p_sp, temp, on='Date', how='left')\n",
    "            else:\n",
    "                temp = p\n",
    "                temp.columns = [feature+'_'+i if i!='Date' else i for i in temp.columns]\n",
    "\n",
    "                print temp.columns\n",
    "                p_sp = pd.merge(p_sp, temp, on='Date', how='left')\n",
    "                \n",
    "        except Exception, e:\n",
    "            print '!!! Error reading ', feature, ', but continue...'\n",
    "            continue\n",
    "            \n",
    "    return p_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_universe = ['YAHOO/SSRI', 'YAHOO/GOLD', 'GOOG/NYSEARCA_CORN', \n",
    "                  'GOOG/NYSEARCA_SOYB', 'CEPEA/SUGAR', 'CHRIS/CME_SP1', \n",
    "                  'YAHOO/INDEX_DJI', 'NASDAQOMX/NQGI', 'MWORTH/9_0', 'MWORTH/2_0', \n",
    "                  'CURRFX/USDEUR', 'CURRFX/USDJPY', 'CURRFX/USDGBP', 'CURRFX/USDCNY', 'YAHOO/INDEX_GDAXI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP index acquired\n",
      "READING  YAHOO/SSRI\n",
      "Adjusted Close\n",
      "READING  YAHOO/GOLD\n",
      "Adjusted Close\n",
      "READING  GOOG/NYSEARCA_CORN\n",
      "Close\n",
      "READING  GOOG/NYSEARCA_SOYB\n",
      "Close\n",
      "READING  CEPEA/SUGAR\n",
      "Cash Price (USD)\n",
      "READING  CHRIS/CME_SP1\n",
      "Index([u'Date', u'CHRIS/CME_SP1_Open', u'CHRIS/CME_SP1_High',\n",
      "       u'CHRIS/CME_SP1_Low', u'CHRIS/CME_SP1_Last', u'CHRIS/CME_SP1_Change',\n",
      "       u'CHRIS/CME_SP1_Settle', u'CHRIS/CME_SP1_Volume',\n",
      "       u'CHRIS/CME_SP1_Open Interest'],\n",
      "      dtype='object')\n",
      "READING  YAHOO/INDEX_DJI\n",
      "Adjusted Close\n",
      "READING  NASDAQOMX/NQGI\n",
      "Index([u'NASDAQOMX/NQGI_Trade Date', u'NASDAQOMX/NQGI_Index Value',\n",
      "       u'NASDAQOMX/NQGI_High', u'NASDAQOMX/NQGI_Low',\n",
      "       u'NASDAQOMX/NQGI_Total Market Value',\n",
      "       u'NASDAQOMX/NQGI_Dividend Market Value'],\n",
      "      dtype='object')\n",
      "!!! Error reading  NASDAQOMX/NQGI , but continue...\n",
      "READING  MWORTH/9_0\n",
      "Index([u'MWORTH/9_0_Year', u'MWORTH/9_0_Value'], dtype='object')\n",
      "!!! Error reading  MWORTH/9_0 , but continue...\n",
      "READING  MWORTH/2_0\n",
      "Index([u'MWORTH/2_0_Year', u'MWORTH/2_0_Value'], dtype='object')\n",
      "!!! Error reading  MWORTH/2_0 , but continue...\n",
      "READING  CURRFX/USDEUR\n",
      "Index([u'Date', u'CURRFX/USDEUR_Rate', u'CURRFX/USDEUR_High (est)',\n",
      "       u'CURRFX/USDEUR_Low (est)'],\n",
      "      dtype='object')\n",
      "READING  CURRFX/USDJPY\n",
      "Index([u'Date', u'CURRFX/USDJPY_Rate', u'CURRFX/USDJPY_High (est)',\n",
      "       u'CURRFX/USDJPY_Low (est)'],\n",
      "      dtype='object')\n",
      "READING  CURRFX/USDGBP\n",
      "Index([u'Date', u'CURRFX/USDGBP_Rate', u'CURRFX/USDGBP_High',\n",
      "       u'CURRFX/USDGBP_Low'],\n",
      "      dtype='object')\n",
      "READING  CURRFX/USDCNY\n",
      "Index([u'Date', u'CURRFX/USDCNY_Rate', u'CURRFX/USDCNY_High (est)',\n",
      "       u'CURRFX/USDCNY_Low (est)'],\n",
      "      dtype='object')\n",
      "READING  YAHOO/INDEX_GDAXI\n",
      "Adjusted Close\n"
     ]
    }
   ],
   "source": [
    "#Grab other stock data\n",
    "stocks_tang = get_features_close_price(stock_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks_tang.to_csv('/Users/Kevin/Desktop/project/fundamendal_analysis_data/tang_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "s2 = get_stock('MWORTH/9_0',trim_start = \"1993-01-01\", trim_end = \"2015-10-31\").reset_index()\n",
    "s3 = get_stock('MWORTH/2_0',trim_start = \"1993-01-01\", trim_end = \"2015-10-31\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
